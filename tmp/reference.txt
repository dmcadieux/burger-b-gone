0 Progress  

1 Monitoring 

2 Guide

3 Table of Contents 

4 Introduction ........................................................................................................................................................ 1 

5 Foundations of Progress Monitoring ............................................................................................................... 1 

6 Deciding Which Students and Measures to Monitor .................................................................................... 3 

7 Performance Levels.......................................................................................................................................... 4 

8 Instructional Tiers ............................................................................................................................................ 5 

9 Determining Intervention Needs, Kindergarten and Grade 1 ....................................................................... 6 

10 Determining Intervention Needs, Grades 2 Through 8 ................................................................................. 7 

11 Use With Learning Disability Qualification ..................................................................................................... 9 

12 Selecting the Grade Level of Monitoring Materials ..................................................................................... 10 

13 Creating a Progress Monitoring Schedule .................................................................................................... 11 

14 Step 1: Select a Baseline Score ...................................................................................................................... 11 

15 Step 2: Select a Goal Date ............................................................................................................................. 12 

16 Step 3: Set a Goal Score ................................................................................................................................ 12 

17 Goal Setting With Individual Education Plans ............................................................................................... 13 

18 Evaluating Progress .......................................................................................................................................... 14 

19 Case Studies ...................................................................................................................................................... 16 

20 Case Study 1: Early Numeracy, Kindergarten, On-Grade ........................................................................... 16 

21 Case Study 2: Early Literacy, Grade 1, Off-Grade ....................................................................................... 17 

22 Case Study 3: Oral Reading Fluency, Grade 3, On-Grade .......................................................................... 19 

23 Case Study 4: Number Sense Fluency, Grade 6, Off-Grade ....................................................................... 19 

24 References ......................................................................................................................................................... 22 

25 Appendix A: Student Growth Percentiles .................................................................................................... 23 

26 Appendix B: Visual Analysis ............................................................................................................................ 25 

27 Tables 

28 Table 1  Sequence of Early Literacy and Early Numeracy CBMs, by Grade and Season ............................. 7 

29 Table 2  Common At-Risk Score Profiles and Recommendations for Next Steps, Reading ....................... 8 

30 Table 3  Common At-Risk Score Profiles and Recommendations for Next Steps, Math ............................ 8 

31 Table 4  Number Comparison Fluency–Triads Item Counts, by Skill Area and Grade ............................. 21 

32 Table 5  Mental Computation Fluency Item Counts, by Skill Area and Grade ........................................... 21 

33 Figures 

34 Figure 1  Benchmark Comparison Screen ...................................................................................................... 3 

35 Figure 2  Oral Reading Fluency National Norms Table, Grade 2 ................................................................. 9 

36 Figure 3  Survey Level Assessment Screen ................................................................................................... 11 

37 aimswebPlus 

38 For more information visit aimswebPlus.com 

39 |   i   | 

40  

41 Progress Monitoring Guide 

42 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

43 Table of Contents cont. 

44 Figures cont. 

45 Figure 4  Progress Monitoring Screen, Baseline Scores ............................................................................... 12 

46 Figure 5  Progress Monitoring Screen, Target Week & Monitor Frequency .............................................. 12 

47 Figure 6  Progress Monitoring Screen, Goal Score ...................................................................................... 13 

48 Figure 7  Sample Progress Monitoring Score Graph, Oral Reading Fluency ............................................... 15 

49 Figure 8  Progress Monitoring Score Graph at Week 6, Case Study 1 ....................................................... 17 

50 Figure 9  Progress Monitoring Score Graph at Week 12, Case Study 1 ..................................................... 17 

51 Figure 10  Survey Level Assessment Scores, Case Study 2.......................................................................... 18 

52 Figure 11  Progress Monitoring Score Graph at Week 7, Case Study 2 ..................................................... 19 

53 Figure B1  Projection Line With Outlier ....................................................................................................... 25 

54 Figure B2  Projection Line With Outlier Removed ...................................................................................... 25 

55 Figure B3  Projection Line With Nonlinear Pattern of Score Growth ........................................................ 26 

56 Figure B4  Score Pattern With Low Variability ............................................................................................. 26 

57 Figure B5  Score Pattern With High Variability ............................................................................................ 27 

58  

59 aimswebPlus 

60 For more information visit aimswebPlus.com 

61 |   ii   | 

62  

63 Progress Monitoring Guide 

64 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

65 Introduction 

66 aimswebPlus® is an assessment, data management, and reporting system that provides national and local 

67 performance and growth norms for the screening and progress monitoring of math and reading skills for all 

68 students in Kindergarten through Grade 8. aimswebPlus uses two types of measures: curriculum-based 

69 measures (CBMs)—brief, timed measures of fluency on essential basic skills—and standards-based 

70 assessments (SBAs), which are comprehensive measures aligned to current learning standards. By 

71 combining these two types of measures, aimswebPlus provides the data that schools need for program 

72 planning and evaluation and for tiered assessment (multi-tiered system of supports [MTSS], also known as 

73 response to intervention [RTI]). aimswebPlus data provide the information needed to differentiate 

74 instruction and determine who will benefit from intensive intervention. 

75  

76 Progress monitoring—the frequent and ongoing collection of information about student performance—is 

77 an essential component of a tiered assessment system. By collecting such data, a student’s rate of 

78 improvement can be compared to expected growth to determine if adequate progress is being made 

79 toward end-of-year goals or if changes to instruction are needed to improve student growth. 

80  

81 In this guide, the foundations of progress monitoring are first discussed, followed by sections on identifying 

82 which students to progress monitor, selecting the measure(s) to monitor and the appropriate grade level 

83 of monitoring materials, and creating a progress monitoring schedule. Next, case studies are presented  

84 to illustrate progress monitoring within the aimswebPlus system. Finally, information regarding student 

85 growth percentiles and conducting a visual analysis of progress monitoring data are presented in 

86 Appendixes A and B, respectively. 

87  

88 Foundations of Progress Monitoring 

89 The Individuals with Disabilities Education Act (IDEA, 2004) and the Every Student Succeeds Act (ESSA, 

90 2015) require schools to evaluate the effects of evidence-based instruction, a task for which RTI is ideally 

91 suited. Monitoring each student’s response to intervention enables teachers to gauge the effectiveness  

92 of the core curriculum and to align resources with student needs. Studies indicate that using progress 

93 monitoring to guide instruction improves student achievement, especially for students with low 

94 achievement and those with disabilities, when qualified professionals implement the following  

95 program features (Black & Wiliam, 1998; Fuchs & Fuchs, 1986; Stecker, Fuchs, & Fuchs, 2005): 

96 •  Collect data frequently 

97 •  Display and review data graphically 

98 •  Create explicit decision rules for when to continue or modify instruction 

99 • 

100 •  Provide clear direction for instructional modifications and alternatives 

101 Implement data collection and decision rules with integrity 

102 The aimswebPlus system supports accurate and frequent data collection, automatically displays results and 

103 trends via graphs, projects growth trends alongside growth expectations, and provides explicit rules for 

104 deciding whether a student is on or off track to meet his or her end-of-year goals. Note that the last 

105 feature of effective progress monitoring, providing direction on how to modify or replace instructional 

106 interventions, is beyond the scope of this guide. Progress monitoring can help to determine if an 

107 instructional program is effective, but it cannot identify potential alternatives. 

108  

109  

110 aimswebPlus 

111 For more information visit aimswebPlus.com 

112 |   1   | 

113  

114 Progress Monitoring Guide 

115 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

116 The aimswebPlus approach to frequent progress monitoring relies on curriculum-based measurement, a 

117 method that is standardized, valid, reliable, repeatable, simple, efficient, inexpensive, and sensitive to 

118 achievement growth over relatively brief intervals (Deno, 1986). Curriculum-based measures (CBMs) 

119 assess foundational skills that are the basis for success on higher-order, complex skills. With appropriate 

120 instruction, basic skills can rapidly develop and this development will be represented by improved student 

121 performance on CBMs. These characteristics make CBMs ideally suited to the frequent assessment that is 

122 required for progress monitoring programs (Deno, 1985, 2003). Together, triannual screening and 

123 progress monitoring link expectations (i.e., benchmarks) and performance levels (i.e., norms) to individual 

124 student goals. 

125  

126 The aimswebPlus standards-based assessments (SBAs) measure the higher-order thinking skills and 

127 conceptual knowledge defined in current learning standards. SBAs include reading comprehension, 

128 vocabulary knowledge, and math concepts and applications. Although students may have deficits in any 

129 one of these domains, frequent progress monitoring on these specific content areas is not recommended 

130 because complex, higher-order thinking skills develop more slowly than foundational skills and reliable 

131 assessment of these complex skills requires more testing time than is reasonable on a weekly basis. As 

132 such, the Concepts & Applications and Reading Comprehension measures are not available for weekly 

133 progress monitoring. 

134  

135 Real growth—that is, growth that is reliably greater than zero on a standardized assessment—in these 

136 domains typically takes several months of intervention to detect. One of the benefits of triannual interim 

137 assessment (i.e., benchmarking) is that data can be used to identify growth in these domains without 

138 waiting an entire school year for end-of-year assessment results. These triannual, standards-based 

139 assessments can also be supplemented with unit quizzes, practice problems, and/or mastery monitoring 

140 methods widely available in reading and math intervention systems. 

141  

142 Within an RTI system, student performance is organized into three tiers. Students in Tier 3 typically need 

143 intensive intervention to build skills and deepen their understanding of learning standards taught in the core 

144 instructional program. Students in Tier 2 may need only small-group instruction and additional practice on 

145 core content to get on track. Students in Tier 1 are expected to stay on track with high quality, research-

146 based core instruction. 

147  

148 For students in Tier 3, frequent (i.e., weekly) monitoring of progress is recommended because it can 

149 shorten the time needed to make an informed decision about the effectiveness of the chosen intervention. 

150 Delaying that decision may put the student at further risk of failure. For students needing less intensive 

151 intervention, it is appropriate to monitor progress less frequently than weekly because these students’ 

152 growth tends to be less rapid; as a result, increased time between assessments is needed to accurately 

153 project whether a student is on track to meet end-of-year goals. For these students, increasing assessment 

154 frequency will do little to improve the accuracy of the projection or decisions about the effectiveness of 

155 the intervention. 

156  

157 Progress monitoring is an important part of a dynamic approach to guiding instruction and interventions for 

158 both groups and individuals. Rather than plan and hope, progress monitoring provides an opportunity to 

159 plan and evaluate whether curriculum and instructional procedures are increasing the rate of student 

160 progress towards explicit goals. 

161  

162 aimswebPlus 

163 For more information visit aimswebPlus.com 

164 |   2   | 

165  

166 Progress Monitoring Guide 

167 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

168 Progress monitoring involves several basic activities, including: 

169 •  deciding which students to progress monitor and which CBMs to use, 

170 • 

171 • 

172 • 

173 •  deciding whether progress is adequate and whether the instruction and/or the goal should be 

174 selecting the grade level of the CBMs, 

175 setting a goal and creating a schedule for data collection, 

176 collecting data (i.e., administering CBMs) and evaluating progress, 

177 modified, and 

178 •  deciding if the goal has been reached. 

179  

180 Deciding Which Students and Measures to Monitor 

181 A student’s teacher typically makes the decisions about whose progress to monitor, on which measures, 

182 and how frequently they should be assessed, with input from reading and/or math specialists or other 

183 professionals with knowledge of the student’s needs. The decision to progress monitor is not an automatic 

184 process based solely on test scores; rather, teachers should utilize aimswebPlus scores and other evidence 

185 (e.g., classroom performance) to guide decisions and identify the intensity of the need. 

186  

187 The aimswebPlus system provides several types of scores and reports to aid in this decision process. The 

188 benchmark comparison screen (see Figure 1) and the student profile page provide the most useful 

189 information for deciding if a student needs intensive intervention and progress monitoring. The benchmark 

190 comparison screen displays reading or math composite scores, as well as scores on each measure 

191 completed by each student. Performance can be viewed by score, local or national percentiles, or tiers. 

192 When a CBM score is at or below the 25th national percentile, a goal setting icon (     ) is activated next to 

193 the CBM score(s) in question (see Goal column in Figure 1). 

194  

195 Figure 1  Benchmark Comparison Screen 

196  

197 The aimswebPlus system uses tiers and performance levels to guide decisions about whose progress to 

198 monitor and how intense the intervention should be. Each method has benefits and limitations, which are 

199 described in the following sections. Combining information from each method provides the most accurate 

200 assessment of each student’s needs. 

201  

202  

203  

204 aimswebPlus 

205 For more information visit aimswebPlus.com 

206 |   3   | 

207  

208 Progress Monitoring Guide 

209 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

210 Performance Levels 

211 aimswebPlus uses five color-coded performance levels, which are based on either national or local norms: 

212 •  Well-Below Average (1st–10th percentiles), displayed in orange 

213 •  Below Average (11th–25th percentiles), displayed in yellow 

214 •  Average (26th–74th percentiles), displayed in green 

215 •  Above Average (75th–89th percentiles), displayed in teal 

216 •  Well-Above Average (90th–99th percentiles), displayed in blue 

217 Color-coding makes it easier to identify each student’s performance level (see Figure 1). Scores in the 

218 Below and Well-Below Average ranges are far enough below grade-level expectations to indicate a  

219 real achievement gap that could place the student at risk. The aimswebPlus system automatically flags  

220 CBM scores that fall in the Below Average and Well-Below Average range by activating the goal setting 

221 icon (     ) next to the CBM score(s) in question. This is the first indication that a student may require 

222 progress monitoring. If further analysis confirms a need for intensive intervention, then the teacher can 

223 initiate a progress monitoring plan by clicking the goal setting icon. 

224  

225 When using performance levels to determine whose progress should be monitored, it is important to 

226 recognize the differences between national and local norms. National norms are based on large samples  

227 of students who completed Fall, Winter, and Spring benchmark testing during the 2013–14 school year. 

228 This sample was stratified within grade by geographic region, socioeconomic status, sex, race/ethnicity, 

229 special education status, and English Language Learner (ELL) status. National percentiles are highly 

230 accurate and stable, making them a dependable benchmark against which performance across years  

231 can be evaluated. As such, if the goal is to move students toward national benchmarks, use of national 

232 norms is recommended. 

233  

234 Local percentile norms are generated based on samples specified by the user, which may consist of all of 

235 the students at a particular grade level in the user’s state, district, or school. Users can further narrow the 

236 focus of local norm groups by applying selection criteria, such as demographics, disability category, or 

237 instructional program (see the aimswebPlus system Help file for more information). Local norms are 

238 recommended for users who want a fixed percentage of students in the Below Average range. Because 

239 local norms adjust for local performance levels, 25% of students will have scores in the Below Average 

240 group. Local norms may be particularly useful for school systems in which performance deviates 

241 substantially, either lower or higher, from the national norm group. However, local norms should be used 

242 with caution if they are based on fewer than 50 students per grade. When sample sizes are small, 

243 performance levels are unstable and may fluctuate considerably across seasons and years. 

244  

245 One benefit of using performance levels is that they communicate student need in a familiar manner.  

246 For example, teachers and parents easily understand that Below Average indicates a student is behind 

247 academically and that he or she would benefit from additional help to get back on track. Another benefit of 

248 using performance levels is that they are reported for each measure, enabling educators to quickly identify 

249 skill deficits and strengths for each of their students and to prioritize intervention plans. Incorporating areas 

250 of strength into intervention plans can improve student motivation, which in turn can lead to better results. 

251  

252  

253 aimswebPlus 

254 For more information visit aimswebPlus.com 

255 |   4   | 

256  

257 Progress Monitoring Guide 

258 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

259 Instructional Tiers 

260 Instructional tiers can also be used to identify students who need frequent progress monitoring. In 

261 aimswebPlus, tiers are based on the likelihood of not meeting an end-of-year performance target. A 

262 performance target is the score that all students should achieve by the end of the school year in order to 

263 remain on track for success in subsequent grades. Performance targets are defined by Spring composite 

264 score national percentiles, ranging from the 15th to the 70th percentile in increments of 5. The aimswebPlus 

265 system enables users, typically a school or district administrator, to define different performance targets at 

266 the school and district level for their accounts. A broad range of targets enables schools to select a target 

267 that balances educational needs with available resources. For example, a low-performing school may 

268 choose a lower target to moderate the percentage of students in Tiers 2 and 3. 

269  

270 Once the Spring performance target is defined, the system generates Fall, Winter, and Spring cut scores. 

271 The lower cut score divides Tiers 3 and 2, whereas the upper cut score divides Tiers 2 and 1. Students in 

272 Tier 3 have a high probability of not meeting the selected end-of-year performance target (about 50%–90% 

273 will not meet the target); students in Tier 2 have a moderate probability of not meeting the target (about 

274 25%–65% will not meet the target); and students in Tier 1 have a low probability of not meeting the target 

275 (about 5%–25% will not meet the target). 

276  

277 The advantages of using the aimswebPlus tiers to determine whose progress needs to be monitored 

278 include that they (a) provide a direct indication of risk, (b) are criterion-based, and (c) are not affected by 

279 the varying performance levels of schools and districts. In short, students in Tier 3 are at greatest risk and 

280 likely need intensive intervention to get on track to meet the defined performance target, while Tier 2 

281 students are at moderate risk and may need additional help to achieve the target. 

282  

283 Tiers are derived from composite scores, which typically combine performance from standards-based 

284 assessments and curriculum-based measures. For example, the aimswebPlus Grade 3 Reading composite 

285 comprises the Vocabulary, Reading Comprehension, and Oral Reading Fluency measures. Because 

286 composite scores include both SBAs and CBMs, they provide the broadest and most accurate indicator of 

287 overall reading and math performance. Note that students will typically score at about the same level on all 

288 measures included in a composite, which means a student with a low score on one of the measures will 

289 also have low scores on the other measures. As such, when a composite score falls in Tier 3 (or, Well-

290 Below Average), it is likely that scores on all the measures in the composite will be Below Average or 

291 Well-Below Average. When this occurs, the decision to monitor progress is straightforward. 

292  

293 A limitation of using tiers is that it is possible for a student to have a composite score in Tier 3 and one or 

294 more measure scores in the Average range. For example, a Well-Below Average Reading composite score 

295 and an ORF score in the Average range is indicative of a student who has developed basic reading skills but 

296 is struggling with higher-order thinking skills and concepts. When scores vary across measures (i.e., some 

297 in the Below Average or Well-Below Average range, and others in the Average or Above Average range), 

298 it is important to identify the source of the low performance before determining the best course of action. 

299 As a starting point, review the national percentile rank the student achieved on each measure that 

300 contributed to the composite, looking for the measure(s) with scores in the Well-Below Average and 

301 Below Average ranges. In addition, teachers can use the aimswebPlus Student Skills Plan report to conduct 

302 further analysis of the skill area(s) in which the student’s performance is deficient. 

303  

304  

305 aimswebPlus 

306 For more information visit aimswebPlus.com 

307 |   5   | 

308  

309 Progress Monitoring Guide 

310 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

311 Note that it is also possible for a student to score in the Well-Below Average range on an individual CBM 

312 and have a composite score in the Average range or in Tier 1. This profile indicates that the student is 

313 developing a good foundation on conceptual knowledge and standards-based skills, but is not fluent 

314 and/or lacks automaticity on basic skills. In this case, intensive intervention probably is not necessary, but 

315 the student may benefit from extra practice on these basic skills, and his or her performance in the next 

316 benchmark period should be carefully reviewed to ensure that the gap has not increased. 

317  

318 Finally, be aware that a limitation with strictly applying score ranges or tiers is that they do not differentiate 

319 between scores near the cut score from scores far from the cut score. For example, a student whose 

320 score is just above the cut score for Tier 1 (i.e., at the low end of the tier) or a student whose score is just 

321 above the cut between the Below Average and Average ranges may still need extra help. These students 

322 are more likely to drop into a lower tier (e.g., from Tier 1 to Tier 2) in the subsequent benchmark period 

323 than a student with a score well above the cut score. Because all scores contain some measurement error, 

324 scores near a cut score are within the expected measurement error, which reduces the certainty that the 

325 student truly belongs in a particular tier. When prioritizing resource allocation, consider giving highest 

326 priority to students with the lowest scores. 

327  

328 Determining Intervention Needs, Kindergarten and Grade 1 

329 A different approach to determining intervention needs is recommended in Kindergarten and Grade 1. 

330 aimswebPlus provides several CBMs, each measuring an important and distinct foundational early literacy 

331 or early numeracy skill. Some of the skills follow a common progression, such that one precedes another. 

332 Because the aimswebPlus system flags all CBM scores in the Below Average and Well-Below Average 

333 ranges and because often more than one CBM is flagged for a given student, the first step is to prioritize 

334 the area(s) of greatest need based on day-to-day student performance and classroom observations. 

335  

336 Option 1: Prioritized Progress Monitoring 

337 When scores on more than one measure are in the Well-Below Average range, teachers can prioritize 

338 progress monitoring to focus intervention resources on the area of greatest need first. Table 1 shows Early 

339 Literacy and Early Numeracy CBMs sequenced from easiest (left) to hardest (right), using average Winter 

340 number correct scores as a proxy for cognitive complexity. For instance, Kindergarten students can name 

341 more letters in 1 minute (LNF) than they can produce letter sounds (LWSF). Each sequence shown in 

342 Table 1 roughly approximates the instructional sequence as well (e.g., students are typically taught to 

343 recognize letters before they learn how to make letter sounds). Therefore, if a student’s LNF and LWSF 

344 scores were both Well-Below Average, intervention would initially focus on letter naming and weekly 

345 progress monitoring of LNF. First, an LNF progress monitoring schedule would be established with a goal 

346 date that precedes the next benchmark period. If the student makes adequate progress toward the LNF 

347 goal, then create an LWSF progress monitoring schedule and begin intervening on letter sounds. 

348  

349  

350  

351 aimswebPlus 

352 For more information visit aimswebPlus.com 

353 |   6   | 

354  

355 Progress Monitoring Guide 

356 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

357 Table 1  Sequence of Early Literacy and Early Numeracy CBMs, by Grade and Season 

358 Grade 

359 Battery 

360 Season 

361 CBM 

362 K 

363 K 

364 K 

365 1 

366 1 

367 1 

368 K 

369 K 

370 K 

371 1 

372 1 

373 1 

374 Early Literacy 

375 Fall 

376 IS, LNF, LWSF 

377 Early Literacy 

378 Winter 

379 IS, LNF, LWSF, PS 

380 Early Literacy 

381 Spring 

382 LNF, LWSF, PS, WRF 

383 Early Literacy 

384 Fall 

385 LWSF, PS, WRF, ORF 

386 Early Literacy 

387 Winter  WRF, ORF 

388 Early Literacy 

389 Spring  WRF, ORF 

390 Early Numeracy 

391 Fall 

392 NNF, QTF 

393 Early Numeracy 

394 Winter  NNF, QTF, QDF 

395 Early Numeracy 

396 Spring 

397 NNF, QTF, QDF 

398 Early Numeracy 

399 Fall 

400 NCF–P, MFF–1D 

401 Early Numeracy 

402 Winter  NCF–P, MFF–1D,  MFF–T 

403 Early Numeracy 

404 Spring 

405 NCF–P, MFF–1D,  MFF–T 

406  

407 Not all skills are acquired in such clear sequence. For example, young learners typically practice naming 

408 numbers at about the same time they begin to count objects. However, number recognition advances 

409 more rapidly than counting for most learners. As such, it may be better to focus first on number 

410 recognition. One intervention technique is to pair rote counting, something young learners pick up quickly 

411 (especially when associated with a simple melody), with printed number lists to accelerate number 

412 recognition growth. Once the student has demonstrated adequate recognition of single-digit numbers, a 

413 scaffolded strategy can be used to help the student count objects in sets by pairing the objects with printed 

414 numbers. Note that these examples are provided to give users a general sense of how prioritization can 

415 work with aimswebPlus. A full list of strategies is beyond the scope of this guide. 

416  

417 Option 2: Simultaneous Progress Monitoring 

418 With the simultaneous progress monitoring approach, progress monitoring and intervention begin 

419 simultaneously on every skill that falls in the Well-Below Average range. This is appropriate if there is 

420 sufficient time allocated to intervene on multiple skills (e.g., NCF–P and MFF–1D). As the intervention 

421 proceeds, progress in each area should be compared to determine whether the student is progressing 

422 adequately and relatively equally on each skill. If growth on any skill is substantially below the aimline, 

423 consider refocusing the intervention on that skill. 

424  

425 Determining Intervention Needs, Grades 2 Through 8 

426 Overall, it is recommended that aimswebPlus tiers and performance levels be used to determine which 

427 students need intensive or supplemental intervention. Intensive intervention and weekly progress 

428 monitoring (PM) is recommended for students in Tier 3 or in the Well-Below Average range on the 

429 Reading or Math composite, as well as for students in the Well-Below Average or Below Average range on 

430 any one Reading or Math CBM. Tables 2 and 3 (Reading and Math, respectively) list scenarios in which a 

431 composite score, a CBM, or both are Well-Below Average and the recommended next steps. These 

432 tables also indicate the likelihood of each scenario, based on the aimswebPlus national norms database. 

433  

434  

435 aimswebPlus 

436 For more information visit aimswebPlus.com 

437 |   7   | 

438  

439 Progress Monitoring Guide 

440 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

441 Table 2  Common At-Risk Score Profiles and Recommendations for Next Steps, Reading 

442 Frequency 

443 Next steps 

444 Subject  Grades 

445 Composite 

446 Reading 

447 2–3 

448 Tier 3 or 

449 Well-Below Avg 

450 CBM 

451 performance 

452 Well-Below Avg 

453 to Below Avg 

454 10% 

455 Reading 

456 2–3 

457 Tier 3 or 

458 Well-Below Avg 

459 Avg or higher 

460 1% 

461 Reading 

462 2–3 

463 Tier 1 or Avg  Well-Below Avg 

464 1% 

465 Reading 

466 2–3 

467 Reading 

468 4–8 

469 Tier 2 or 

470 Below Avg 

471 Tier 3 or 

472 Well-Below Avg 

473 Below Avg 

474 6% 

475 Well-Below Avg 

476 to Below Avg 

477 7–8% 

478 Reading 

479 4–8 

480 Tier 3 or 

481 Well-Below Avg 

482 Avg or higher 

483 1–2% 

484 Reading 

485 4–8 

486 Tier 1 or Avg  Well-Below Avg 

487 1–3% 

488 Reading 

489 4–8 

490 Tier 2 or 

491 Below Avg 

492 Below Avg 

493 4–6% 

494 Implement intensive reading intervention and 

495 weekly PM with ORF. 

496 Evaluate performance on individual reading 

497 measures and review the Student Skills Plan 

498 report; provide additional reading support to 

499 build vocabulary and improve reading 

500 comprehension strategies. 

501 Reassess ORF below grade level; consider 

502 providing additional practice with oral 

503 reading and grade-level word identification. 

504 Supplement core instruction; reevaluate at 

505 the next benchmark. 

506 Implement intensive reading intervention and 

507 weekly PM with SRF. 

508 Evaluate performance on individual reading 

509 measures and review the Student Skills Plan 

510 report; provide additional reading support to 

511 build vocabulary and improve reading 

512 comprehension strategies. 

513 Reassess SRF below grade level; consider 

514 providing additional reading practice on main 

515 idea, using texts at or below grade level. 

516 Supplement core instruction; reevaluate at 

517 the next benchmark. 

518  

519  

520 Table 3  Common At-Risk Score Profiles and Recommendations for Next Steps, Math 

521 Subject  Grades 

522 Composite 

523 Math 

524 2–8 

525 Tier 3 or 

526 Well-Below Avg 

527 CBM 

528 performance 

529 Well-Below Avg 

530 to Below Avg 

531 Frequency 

532 Next steps 

533 8–10% 

534 Implement intensive math intervention and 

535 weekly PM with NSF. 

536 Math 

537 2–8 

538 Tier 3 or 

539 Well-Below Avg 

540 Avg or higher 

541 1–2% 

542 Math 

543 2–8 

544 Tier 1 or Avg  Well-Below Avg 

545 1% 

546 Math 

547 2–8 

548 Tier 2 or 

549 Below Avg 

550 Below Avg 

551 5–7% 

552  

553  

554  

555 Evaluate performance on individual math 

556 measures and review the Student Skills Plan 

557 report; provide additional math support to 

558 build conceptual knowledge and problem 

559 solving strategies. 

560 Reassess NSF below grade level; consider 

561 providing additional practice on number 

562 sense and computational skills. 

563 Supplement core instruction; reevaluate at 

564 the next benchmark. 

565 aimswebPlus 

566 For more information visit aimswebPlus.com 

567 |   8   | 

568  

569 Progress Monitoring Guide 

570 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

571 Use With Learning Disability Qualification 

572 The reauthorization of IDEA in 2004 included provisions for using an RTI-based approach as part of 

573 evaluating for specific learning disabilities (34 CFR 300.309; IDEA, 2004), often referred to as the dual-

574 discrepancy model. With this model, a student may qualify as having a specific learning disability if the 

575 following conditions are met: 

576 •  The student does not achieve adequately to meet grade-level standards when provided with 

577 learning experiences and instruction appropriate for the child’s grade level. 

578 •  The student does not make sufficient progress to meet grade-level standards when given scientific, 

579 research-based interventions in one or more of the eight learning areas. 

580 Detailed guidance, which varies across states, is left to local education agencies. However, in some states, 

581 guidance may simply restate the dual-discrepancy model (i.e., IDEA provision 34 CFR 300.309). In the 

582 absence of explicit performance standards for each of these conditions, follow the guidelines provided 

583 below when using the aimswebPlus system. 

584  

585 aimswebPlus provides results for five learning areas: basic reading skills, reading fluency, reading 

586 comprehension, math computation, and math problem solving. Performance in the Well-Below Average 

587 range (1st–10th national percentile) on any reading or math measure indicates a significant performance 

588 deficit. Insufficient growth is indicated by a Fall-to-Winter, Winter-to-Spring, or Fall-to-Spring rate of 

589 improvement (ROI) on the corresponding SBA or CBM that is below the national average for that grade, 

590 season, and measure. The national norms tables available in the aimswebPlus platform (see Figure 2) 

591 provide median ROIs for all required measures and can be used to determine if progress is adequate. 

592  

593 Figure 2  Oral Reading Fluency National Norms Table, Grade 2 

594  

595  

596 As previously stated, to be effective for weekly progress monitoring, measures must be brief, reliable, and 

597 sensitive to growth over relatively short intervals of time. To be sensitive to growth, the skills assessed 

598 must develop fairly rapidly and lend themselves to brief administration. Complex skills—those that involve 

599 reasoning and analysis—develop more slowly. For example, math word problems require reading, 

600 analyzing the knowns and unknowns, identifying a strategy, and solving the problem, which often involves 

601 computation. As such, the aimswebPlus standards-based measures of Concepts & Applications and 

602 Reading Comprehension are assessed during triannual interim assessment (i.e., benchmarking) and are not 

603 available for weekly progress monitoring. 

604  

605  

606 aimswebPlus 

607 For more information visit aimswebPlus.com 

608 |   9   | 

609  

610 Progress Monitoring Guide 

611 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

612 Selecting the Grade Level of Monitoring Materials 

613 After the decision has been made to monitor a student’s progress and the measures to use have been 

614 selected, the next step is to determine the grade level of materials to use. Students at risk, especially those 

615 in Tier 3, are usually performing below grade level. As noted by Shapiro (2008, p. 48), “A student who is 

616 functioning below enrolled grade level will demonstrate little progress over time if monitored at levels that 

617 exceed his or her instructional level” and should consequently “be measured at his or her highest 

618 instructional level”. In other words, monitoring progress using on-grade level materials may be 

619 inappropriate because those forms may be too difficult for a student functioning below grade level and may 

620 not reflect the content and skills most suited to the student’s learning needs. 

621  

622 Determining the grade level for a progress monitoring goal should begin by analyzing the student’s 

623 performance level on the composite and on the individual aimswebPlus measures. As previously described, 

624 frequent progress monitoring is recommended for students with composite scores in the Well-Below 

625 Average range and CBM scores in the Below Average and Well-Below Average ranges. The composite 

626 scores provide the broadest coverage of reading and math skills and are the most predictive of long-term 

627 risk. The CBM scores indicate possible deficits on basic skills and fluency. 

628  

629 The aimswebPlus system has five performance levels (i.e., Well-Below Average through Well-Above 

630 Average) that characterize the level of need. When using these performance levels to analyze student 

631 needs, please keep in mind the following: 

632 •  Below Average indicates that the student is struggling with some or many on-grade level concepts 

633 and skills and probably requires remediation using on-grade content. 

634 •  Well-Below Average indicates that the student has a significant performance deficit and will likely 

635 require remediation on content and skills introduced one to two grades below grade level. 

636 For students in the Well-Below Average range on a composite or a CBM, survey level assessment is 

637 recommended to identify the appropriate grade level for progress monitoring. Survey level assessment 

638 (SLA) refers to a process of establishing the grade level at which to progress monitor a student. More 

639 specifically, SLA is a stepwise procedure in which the student is administered below grade-level CBM 

640 content in successive order, beginning with one grade level below, then two grade levels below and so  

641 on until the student’s performance is above the 25th national percentile. The grade at which this occurs  

642 can be considered the student’s instructional level and the grade level to use when creating a progress 

643 monitoring schedule. 

644  

645 In the aimswebPlus system, follow these steps to select and administer a CBM via the SLA screen (see 

646 Figure 3): 

647 •  Click the New button in the top right corner of the screen. 

648 • 

649 In the Assess pop-up window that appears, select the measure using the Measure drop-down 

650 menu and the grade using the Grade drop-down menu. A form will be automatically assigned. 

651 For measures assessed via digital record form (DRF), click the Assess button to launch the DRF.  

652 For measures assessed online, click the Assign button. The page will reload and display information 

653 about the form assigned, including a ticket icon in the Score column. Click the ticket icon to obtain 

654 the TestNav username and password for the student. 

655 • 

656 • 

657  

658  

659 aimswebPlus 

660 For more information visit aimswebPlus.com 

661 |   10   | 

662  

663 Progress Monitoring Guide 

664 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

665 Figure 3  Survey Level Assessment Screen 

666  

667  

668 As experience with aimswebPlus increases, teachers and interventionists will develop expertise in 

669 interpreting performance patterns and identifying how those patterns relate to student needs. When that 

670 level of expertise is developed, it may be appropriate to forego a stepwise approach and go directly to the 

671 optimal grade level for progress monitoring. 

672  

673 Creating a Progress Monitoring Schedule 

674 Creating a progress monitoring schedule consists of three basic components: a baseline score, a goal date, 

675 and a goal score. The baseline score is the first score included in the monitoring schedule. For most 

676 students who are monitored on-grade level, the benchmark score is used as the baseline. For students 

677 who are monitored off-grade level, the baseline score is obtained through the survey level assessment 

678 process (see previous section). The goal date is the date at which the progress monitoring schedule 

679 expires, and the goal score is the desired level of performance the student should achieve by the goal date. 

680 Typically, progress monitoring schedules are paired with a scientifically validated intervention plan. 

681  

682 The following steps describe a systematic process for creating an aimswebPlus progress monitoring 

683 schedule, as well as important considerations and recommendations to keep in mind. 

684  

685 Step 1: Select a Baseline Score 

686 First, select a baseline score from the list of available scores (see Figure 4). This list includes all of the 

687 student’s scores for a given CBM measure at a given grade level, including benchmark and progress 

688 monitoring scores. This list also shows the date of each score, the corresponding national percentile, and 

689 the total raw score. Select the baseline score at or just prior to the start of the intervention (i.e., the 

690 additional instructional support provided to the student to get her or him back on track). 

691  

692  

693 aimswebPlus 

694 For more information visit aimswebPlus.com 

695 |   11   | 

696  

697 Progress Monitoring Guide 

698 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

699 Figure 4  Progress Monitoring Screen, Baseline Scores 

700  

701  

702 Step 2: Select a Goal Date 

703 Next, select a goal date. The goal date can be set up to 1 year from the baseline date and should allow 

704 enough time to adequately assess the effectiveness of an intervention. It is recommended that users set 

705 goal dates at least 16 weeks after the start of the intervention. Because the aimswebPlus system uses 

706 growth norms to aid in goal setting and these growth norms are based on within-year growth (i.e., Fall to 

707 Spring), it is recommended that users set the goal date within the same school year as the baseline score. 

708  

709 It is common practice to begin interventions shortly after Fall benchmark testing and to define a Spring goal 

710 date. The goal date can be selected in the system by moving the slider or by clicking the − and + symbols 

711 (see Figure 5) in the Target Week & Monitor Frequency section. The default progress monitoring 

712 frequency is weekly testing. For less intensive interventions, choose a monthly testing schedule from the 

713 drop-down menu. As the slider moves, the goal date automatically adjusts. 

714  

715 Figure 5  Progress Monitoring Screen, Target Week & Monitor Frequency 

716  

717  

718 Step 3: Set a Goal Score 

719 Lastly, set a goal score. Select the goal score by moving the slider or by clicking the − and + symbols  

720 (see Figure 6) in the Goal Score section. Each click of the − or + symbol adjusts the goal score by  

721 1 raw score point. 

722  

723  

724 aimswebPlus 

725 For more information visit aimswebPlus.com 

726 |   12   | 

727  

728 Progress Monitoring Guide 

729 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

730 Figure 6  Progress Monitoring Screen, Goal Score 

731  

732 As the slider moves, the system provides feedback about the appropriateness of the goal. Four feedback 

733 levels, based on student growth percentiles (SGPs), are used to define the ambitiousness of the goal. 

734  

735 These feedback levels are defined as follows: 

736  

737 • 

738 Insufficient (SGP = 49 and below): The ROI is below the national average and will not sufficiently 

739 close the gap between a student’s current score and the goal score. 

740 •  Closes the Gap (SGP = 50–85): The ROI is above the national average and will improve the 

741 student’s score and corresponding percentile rank; however, it may not reflect the potential 

742 growth rate that can be achieved with a high quality, intensive intervention. 

743 •  Ambitious (SGP = 86–97): The ROI is well above the national average and will substantially 

744 • 

745 improve the student’s score and corresponding percentile rank. 

746 Extremely Ambitious (SGP = 98 and above): The ROI is greater than 97% of those in the national 

747 sample of students with a similar initial score level; such an ROI is rarely achieved in typical 

748 classroom settings. 

749 Because feedback changes at each cut point between levels, the change can happen quickly. Click the − or 

750 + symbol to find the precise score that corresponds to each transition. 

751  

752 New users should define a goal at the bottom of the Ambitious range. This range begins at an SGP  

753 of 86 and is just above the top end of the Closes the Gap range. An SGP of 86 represents a growth  

754 rate achieved by just 15% of the national sample, which is why it is considered ambitious. However,  

755 it is reasonable to expect significantly higher than average growth when implementing effective,  

756 intensive intervention. 

757  

758 For very experienced users or when using an external criterion based on the raw score or national or local 

759 percentiles, simply select the criterion score and save the goal. If the criterion corresponds to the 

760 Insufficient or the Extremely Ambitious range, consider adjusting the goal. For more information about 

761 student growth percentiles, see Appendix A of this guide. 

762  

763 Goal Setting With Individual Education Plans 

764 For students on Individual Education Plans (IEPs), it is customary to set a goal date about 1 year from the 

765 date of the baseline score. aimswebPlus can accommodate progress monitoring schedules that extend  

766 into the next school year. However, it is important to note that the slider in the Goal Score section  

767 aimswebPlus 

768 For more information visit aimswebPlus.com 

769 |   13   | 

770  

771 Progress Monitoring Guide 

772 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

773 (see Figure 6) uses end-of-year Spring norms at the grade level selected for monitoring materials. For 

774 example, if a progress monitoring schedule using Grade 6 NSF forms extends from one school year to  

775 the next, the percentile shown in the Goal Score section is based on Grade 6 Spring norms. 

776  

777 In some situations, it may be more appropriate to divide the goal into parts. This may occur when a 

778 student is well-below grade level and the goal is to move the student to on-grade level performance. Even 

779 with effective intensive intervention, such a goal may be too ambitious to be accomplished in a single 

780 school year. Subdividing long-term goals benefits the teacher and the student. Generally, these students 

781 need to begin with below grade-level materials; as such, it is most useful to establish an interim goal on the 

782 current content for a given school year and then a subsequent goal when the new school year begins. This 

783 allows users to benefit from the use of goal feedback derived from national growth norms. 

784  

785 Evaluating Progress 

786 aimswebPlus generates graphical displays of progress monitoring results, which can be accessed via the 

787 individual student reports within the system. These progress monitoring charts enable teachers to evaluate 

788 each student’s progress on the measures being monitored and to determine whether progress is adequate, 

789 whether instruction and/or the goal should be modified, and if the goal has been met. 

790  

791 With each administration of a progress monitoring form, the student’s total score is plotted on the 

792 progress monitoring chart for that measure. Each measure’s total score is calculated the same way for 

793 benchmark and progress monitoring forms, making it easy to compare performance across the year. For 

794 example, the Oral Reading Fluency total score is equal to the number of words read correctly in 1 minute. 

795 Number Comparison Fluency–Triads and Mental Computation Fluency employ a correction for guessing 

796 when calculating each measure’s total score: number correct minus one-half the number incorrect. 

797 Together, these measures’ scores combine into a Number Sense Fluency (NSF) total score, which is the 

798 simple sum of the NCF–T and MCF corrected scores. 

799  

800 Figure 7 shows a sample progress monitoring chart for Oral Reading Fluency. The vertical axis represents 

801 performance (i.e., total raw score) and the horizontal axis represents time (i.e., test dates). Each score is 

802 plotted with a black dot and adjacent scores are connected by line segments. The goal score is depicted by 

803 the vertical bar on the right side of the graph, with its height corresponding to the goal score value and its 

804 horizontal position set at the goal date. The black line is called the aimline. It connects the baseline score to 

805 the goal score and provides a visual reference point for comparing progress toward the goal. 

806  

807 The projection line is one of three colors, depending on a student’s progress toward the goal score: green 

808 (projected to meet or exceed goal), gray (projected to be near goal), or pink (projected to not meet goal). 

809 Derived from a least-squares regression model, the projection line is the line of best fit for all of the data 

810 points, indicating the average rate of growth across all available data points. This line extends from the 

811 baseline score to the goal date, approximating the student’s performance at the goal date if the trend 

812 continues. Note that the projection line is included in the chart after the third progress monitoring score is 

813 plotted, including the baseline score. When a clear curvature in the trend is apparent, the linear growth 

814 projection should be interpreted with caution. 

815  

816  

817 aimswebPlus 

818 For more information visit aimswebPlus.com 

819 |   14   | 

820  

821 Progress Monitoring Guide 

822 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

823 Figure 7  Sample Progress Monitoring Score Graph, Oral Reading Fluency 

824  

825  

826 In addition, the aimswebPlus system uses information about the student’s data points (e.g., amount, 

827 variability, time span) to construct a confidence interval for the score projected at the goal date. This 

828 statistically based feedback facilitates decisions about adequate growth and is recommended when the 

829 trend is linear and there are no severe outliers. 

830  

831 A confidence interval is a range of scores that has a certain probability of including a student’s true score—

832 in this case, the student’s actual performance level at the goal date. Centered on the projected score, the 

833 confidence interval’s size (i.e., how far it extends above and below the projected score) is a function of the 

834 amount of error in the projection line. Confidence intervals are smaller when the student’s scores are 

835 tightly clustered around the projection line (i.e., little random variability), when the duration of progress 

836 monitoring is long, and when there are many data points. 

837  

838 aimswebPlus uses the 75% confidence interval as the basis for providing feedback about a student’s likely 

839 outcome. This level of probability was selected because it provides the appropriate balance for the types 

840 of decisions being made. Though it is important to be confident when making decisions to change an 

841 intervention or raise a goal, there is a cost to waiting until the confidence level is too high, particularly 

842 when it means continuing with an ineffective intervention longer than necessary. 

843  

844 Using the relationship between the confidence interval and the goal score, aimswebPlus automatically 

845 generates one of the following statements: 

846 •  Above Target. Projected to meet or exceed the goal. Generated when the lower end of the 

847 confidence interval is above the goal score. 

848 •  Below Target. Projected to not meet the goal. Generated when the upper end of the 

849 confidence interval is below the goal score. 

850 •  Near Target. Projected score at goal date: Between CI Lower End and CI Upper End. Generated 

851 when the goal score is within the confidence interval. 

852 •  Goal Met. Generated when three or more scores exceed the goal score. 

853 •  Goal Not Met. Generated when the goal is not met by the goal date. 

854 • 

855 Indeterminate. Generated when conditions generating a confidence interval have not been met. 

856  

857 The Above Target statement is an indication that the intervention is having the desired effect on learning 

858 and progress monitoring should continue. In some circumstances (e.g., if the original goal was less than 

859 Ambitious), this may indicate that the goal should be raised. The Below Target statement indicates that the 

860 intervention is not having the desired effect on student learning and an intervention change should be 

861 aimswebPlus 

862 For more information visit aimswebPlus.com 

863 |   15   | 

864  

865 Progress Monitoring Guide 

866 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

867 considered. If an Extremely Ambitious goal had been set and sufficient time had elapsed (e.g., at least 8 

868 weeks), this may be a signal that the goal should be lowered. The Near Target statement occurs when the 

869 projected score lies near the goal score or when there is a lot of uncertainty in the projected score (i.e., 

870 the confidence interval is very wide). If the confidence interval is mostly below the goal, it is likely that the 

871 student will not meet the goal, although one cannot state with confidence that this outcome will occur. 

872 Comparing the projection line with the aimline provides the same information via visual representation. 

873 (Note. The report does not display the confidence interval.) 

874  

875 As progress monitoring continues and additional data points are collected, the projected score typically 

876 becomes more accurate, thereby increasing the validity of decisions about whether the student is on track 

877 to meet the goal. The length of time required for an accurate decision varies from student to student, 

878 depending on the number and variability of available scores, the linear or curved nature of the trend, and 

879 how much lower or higher the projection line is than the aimline. 

880  

881 To support this claim, a large data simulation study was conducted to examine some factors that influence 

882 the duration and number of administrations needed to make highly accurate predictions about goal 

883 attainment, as well as to evaluate the accuracy of predicting the true slope from the observed slope. The 

884 principal finding suggests that an average of approximately 14 weekly administrations would be needed to 

885 obtain highly accurate predictions (Christ, Zopluoglu, Long, & Monaghen, 2012); however, this outcome 

886 represents an upper limit of the duration required and does not take into account actual goals. 

887 aimswebPlus researchers evaluated a large sample of actual progress monitoring data and found that 10 to 

888 12 weekly administrations are sufficient to produce highly accurate predictions of goal attainment. 

889 Investigation into this topic is ongoing, with additional feedback and data being collected to better 

890 understand the relationship between duration of progress monitoring and accuracy of predictions. 

891  

892 Historically, multiple methods have been used to interpret the trend of a student’s progress monitoring 

893 scores and to support decisions regarding whether a student is advancing at the desired rate and whether 

894 changes need to be made to the intervention and/or the goal. These methods of time-series analyses 

895 include visually analyzing the pattern of data points, counting consecutive score points relative to the 

896 aimline, and comparing the projection line to the aimline. Note that each method has advantages and 

897 disadvantages. For a discussion of the visual analysis of progress monitoring data, see Appendix B of  

898 this guide. 

899  

900 Case Studies 

901 Four sample case studies are provided here, illustrating how to use aimswebPlus for progress monitoring. 

902 The case studies include use of both on- and off-grade materials from each content area. 

903  

904 Case Study 1: Early Numeracy, Kindergarten, On-Grade 

905 Natalie Katz is a Kindergarten student whose progress is being monitored on Quantity Total Fluency 

906 (QTF). Her progress monitoring plan was established just after the Fall benchmark assessment. Her Early 

907 Numeracy composite score was in the Below Average range, with a national percentile of 24. Natalie 

908 performed well on Number Naming Fluency, naming 32 numbers correctly without errors, which placed 

909 her at the 59th national percentile. Her performance was Below Average on Concepts & Applications, with 

910 a score of 8 and a national percentile of 23. 

911  

912 aimswebPlus 

913 For more information visit aimswebPlus.com 

914 |   16   | 

915  

916 Progress Monitoring Guide 

917 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

918 Natalie’s QTF benchmark score of 7 was also in the Below Average range, corresponding to the  

919 12th national percentile. After 6 weeks of monitoring, the system indicated that at her current growth  

920 rate, Natalie was projected to be below the goal score of 15 at 20 weeks (see Figure 8).  As a result,  

921 her teacher decided to increase the intensity of the intervention and to alter the approach. The new 

922 intervention was a success: Natalie’s QTF scores steadily increased, and by the 12th week she was 

923 projected to meet her goal (see Figure 9). 

924  

925 Figure 8  Progress Monitoring Score Graph at Week 6, Case Study 1 

926  

927  

928 Figure 9  Progress Monitoring Score Graph at Week 12, Case Study 1 

929  

930 Case Study 2: Early Literacy, Grade 1, Off-Grade 

931 Mason Day is a first-grade student with limited English language proficiency. He was administered Oral 

932 Reading Fluency (ORF), Letter Word Sounds Fluency (LWSF), and Auditory Vocabulary (AV) during Fall 

933 benchmarking. His scores were: 

934 •  ORF:  1st percentile (discontinued due to inability to correctly read any words) 

935 • 

936 •  AV:  8th percentile (20 points, 5 errors) 

937 LWSF:  2nd percentile (21 points, 8 errors) 

938 Mason’s Early Literacy composite score was at the 6th percentile, which placed him in Tier 3. This indicates 

939 a high risk of not meeting the end-of-year performance target of the 25th national percentile. 

940  

941 The aimswebPlus system automatically alerted the user to consider progress monitoring Mason on LWSF 

942 and ORF because his scores were below the 25th national percentile. Note that AV is not available for 

943 progress monitoring. Teachers use this measure’s information as an indicator of students’ understanding of 

944 spoken words, language development, and possible vocabulary deficits. 

945  

946 aimswebPlus 

947 For more information visit aimswebPlus.com 

948 |   17   | 

949  

950 Progress Monitoring Guide 

951 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

952  

953  

954 Mason was unable to accurately read any of the highly decodable words in the progressive oral reading 

955 passages and he missed 8 of the 29 letter sounds attempted. For comparison, the average Grade 1 student 

956 during Fall benchmarking accurately says about 45 letter sounds with fewer than three errors and 

957 accurately reads 25 to 30 words per minute on ORF, with two to three errors. Because Mason is not 

958 ready to decode printed words and is struggling with letter sounds, his teacher decided to further evaluate 

959 his foundational reading skills. 

960  

961 The aimswebPlus measures that focus on prereading foundational skills are at the Kindergarten level and 

962 are considered off-grade level in this case. For off-grade level progress monitoring, aimswebPlus provides 

963 the survey level assessment option (see the Selecting the Grade Level of Monitoring Materials section of 

964 this guide for more information, including how-to steps). Mason’s teacher selected Letter Naming Fluency 

965 (LNF) and Phoneme Segmentation (PS) as a starting point. Because these measures are only available in 

966 Kindergarten and all forms are of equal difficulty, it is not necessary to establish a grade level. 

967  

968 Mason’s scores are shown in Figure 10. The table reports Mason’s total score and corresponding national 

969 percentile. The national percentile is based on Kindergarten Spring norms, which is the nearest off-grade 

970 level percentile available for each measure. Because both scores are below the 25th percentile, his teacher 

971 should consider setting up a progress monitoring schedule for both LNF and PS. 

972  

973 Figure 10  Survey Level Assessment Scores, Case Study 2 

974  

975  

976 By default, Mason’s scores earned via survey level assessment are treated as his baseline scores. For LNF, 

977 his teacher set a target week, selected a goal score, and saved Mason’s progress monitoring schedule. 

978 Once saved, Mason’s teacher can view his progress monitoring score graph via the Individual Monitoring 

979 screen in the aimswebPlus system. This graph displays the baseline score, the aimline, the projection line, 

980 the goal score, and indicates progress toward the goal (see Figure 11). 

981  

982 As Mason is administered weekly progress monitoring forms, his scores will be included in the graph and 

983 his projection line will adjust accordingly. Note that the projection line is displayed in gray after three forms 

984 have been administered (including baseline). After at least four forms have been administered and 6 weeks 

985 have elapsed, the projection line is color-coded to indicate progress toward the goal as follows: green 

986 (projected to meet or exceed goal), gray (projected to be near goal), or pink (projected to not meet goal). 

987  

988  

989 aimswebPlus 

990 For more information visit aimswebPlus.com 

991 |   18   | 

992  

993 Progress Monitoring Guide 

994 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

995 Figure 11  Progress Monitoring Score Graph at Week 7, Case Study 2 

996  

997  

998 Case Study 3: Oral Reading Fluency, Grade 3, On-Grade 

999 Sana Kaneko is a third-grade student who transferred to a new school in February. Although Winter 

1000 benchmark testing had been completed, her teacher wanted to determine Sana’s reading and math skills, 

1001 and identify any areas in these subjects that required remediation. Sana completed the reading and math 

1002 benchmark measures at the end of February. 

1003  

1004 Sana performed in the Average range on Concepts & Applications and Above Average on Number Sense 

1005 Fluency. Her reading composite score of 367 placed her at the 26th national percentile, which is at the low 

1006 end of the Average range. On Oral Reading Fluency, she read 70 words correctly per minute, with 10 

1007 errors, placing her at the 15th national percentile. On Vocabulary, her growth scale value score of 175 

1008 placed her at the 43rd national percentile. Finally, Sana’s Reading Comprehension growth scale value score 

1009 of 157 placed her at the 30th national percentile. 

1010  

1011 The school principal had established a school-wide Spring target of the 40th national percentile. With this 

1012 target, Sana’s score was in Tier 2. Based on this information, she is at moderate risk for not reaching the 

1013 target. Given her overall risk level and Below Average performance on ORF, Sana’s teacher placed her on 

1014 strategic monthly monitoring on this measure. 

1015  

1016 Case Study 4: Number Sense Fluency, Grade 6, Off-Grade 

1017 Javier Ortiz is a sixth-grade student who has been struggling with math since third grade. His school 

1018 recently adopted a multi-tiered system of supports (MTSS) approach for identifying students with learning 

1019 disabilities and are in their first year of using aimswebPlus. 

1020  

1021 Javier’s struggles in math were reflected in his Fall benchmark math scores (each score in parentheses): 

1022 •  Math Composite:  4th national percentile (179) 

1023 •  Concepts & Applications:  11th national percentile (178) 

1024 •  Number Sense Fluency:  4th national percentile (1) 

1025 ◦  Mental Computation Fluency:  10th national percentile (1) 

1026 ◦  Number Comparison Fluency–Triads:  4th national percentile (0) 

1027 Note that the math composite score is the sum of CA and NSF scores. Because the composite comprises 

1028 all available information from aimswebPlus about the student’s math ability, it is the most reliable and 

1029 accurate predictor of a student’s expected performance on the end-of-year state math test. However, 

1030 aimswebPlus 

1031 For more information visit aimswebPlus.com 

1032 |   19   | 

1033  

1034 Progress Monitoring Guide 

1035 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1036 because CA is untimed and is designed to provide diagnostic information about areas of strength and 

1037 weakness, it is not practical or meaningful to monitor CA progress on a weekly basis. 

1038  

1039 On the other hand, NSF comprises two brief CBMs that measure critical foundational skills that are highly 

1040 predictive of overall math performance. These measures have correlation coefficients with state math tests 

1041 generally in the 0.70s and 0.80s, and are more sensitive to skill development than CA. NSF combines 

1042 scores from MCF, which measures a student’s ability to solve one- and two-step mental computation 

1043 problems, and NCF–T, which measures a student’s ability to compare numbers within and across number 

1044 systems. Although friendly numbers (e.g., round) were used to make the mental math easier to compute, 

1045 the numbers and operations reflect on-grade level content. 

1046  

1047 Success on the number and operation concepts and skills introduced in Grade 6 depends on a student’s 

1048 successful mastery of prerequisite skills introduced in earlier grades. As such, it is important to consider the 

1049 content of the math CBM when selecting the appropriate grade level of the forms to use for progress 

1050 monitoring. Most students in the Well-Below Average range on the aimswebPlus math composite have 

1051 likely not mastered critical prerequisites introduced from one to three grades below grade level. 

1052 Therefore, off-grade progress monitoring is often recommended for students in this category. 

1053  

1054 Tables 4 and 5 show the topics assessed in NCF–T and MCF, respectively. Note that the Grade 6 forms 

1055 emphasize mental mathematics with fractions. Javier was unable to correctly answer any of the NCF–T 

1056 items and only one MCF item, which indicates that he lacks a basic understanding of fractions. Because of 

1057 this skill deficit, Javier’s teacher decided to start with Grade 4 using the SLA process. Javier’s scores were: 

1058 •  NCF–T:  6 (28th national percentile, relative to Grade 4 norms) 

1059 •  MCF:  7 (31st national percentile, relative to Grade 4 norms) 

1060 •  NSF:  13 (26th national percentile, relative to Grade 4 norms) 

1061 These scores are at the low end of the Average range, suggesting that Javier’s instructional level is Grade 4. 

1062 However, analysis of his performance by topic area via the Student Skills Plan report revealed that Javier 

1063 did not answer any of NCF–T’s 4-digit comparison or fractions items correctly. Moreover, he incorrectly 

1064 answered all of the 4- and 5- digit addition and subtraction items, as well as the items involving division of a 

1065 3-digit multiple of 10 by a 1-digit number. 

1066  

1067 Based on this information, Javier’s teacher opted to administer a Grade 3 NSF form. His scores were:  

1068 •  NCF–T:  15 (58th national percentile, relative to Grade 3 norms) 

1069 •  MCF:  9 (57th national percentile, relative to Grade 3 norms) 

1070 •  NSF:  24 (59th national percentile, relative to Grade 3 norms) 

1071 After examining the Grade 3 results, including performance by topic area, Javier’s teacher determined that 

1072 he needed to develop his understanding of base ten with 3- and 4-digit numbers and to improve his 

1073 addition, subtraction, and basic multiplication skills. Therefore, she decided to set a goal using the Grade 3 

1074 progress monitoring forms. 

1075  

1076  

1077 aimswebPlus 

1078 For more information visit aimswebPlus.com 

1079 |   20   | 

1080  

1081 Progress Monitoring Guide 

1082 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1083 Table 4  Number Comparison Fluency–Triads Item Counts, by Skill Area and Grade 

1084  

1085 Skill area 

1086 2-digit comparisons 

1087 3-digit comparisons 

1088 4-digit comparisons 

1089 5-digit comparisons 

1090 Fractions 

1091 Fractions and decimals 

1092 Decimals 

1093 Negatives 

1094 Scientific notation 

1095 Squared numbers 

1096 2 

1097 17 

1098 23 

1099 -- 

1100 -- 

1101 -- 

1102 -- 

1103 -- 

1104 -- 

1105 -- 

1106 -- 

1107 3 

1108 5 

1109 23 

1110 12 

1111 -- 

1112 -- 

1113 -- 

1114 -- 

1115 -- 

1116 -- 

1117 -- 

1118 Grade 

1119 5 

1120 -- 

1121 -- 

1122 15 

1123 5 

1124 15 

1125 -- 

1126 5 

1127 -- 

1128 -- 

1129 -- 

1130 4 

1131 -- 

1132 10 

1133 20 

1134 -- 

1135 10 

1136 -- 

1137 -- 

1138 -- 

1139 -- 

1140 -- 

1141 6 

1142 -- 

1143 -- 

1144 -- 

1145 -- 

1146 7 

1147 -- 

1148 -- 

1149 -- 

1150 -- 

1151 8 

1152  

1153 -- 

1154 -- 

1155 -- 

1156 24 

1157 16 

1158 14 

1159 8 

1160 8 

1161 -- 

1162 -- 

1163 -- 

1164 8 

1165 8 

1166 8 

1167 -- 

1168 -- 

1169 4 

1170 4 

1171 7 

1172 7 

1173 4 

1174 Item total 

1175 40 

1176 40 

1177 40 

1178 40 

1179 40 

1180 40 

1181 40 

1182  

1183  

1184 Table 5  Mental Computation Fluency Item Counts, by Skill Area and Grade 

1185  

1186 Skill area 

1187 Add and subtract multiples of 10 and 100 

1188 Add and subtract 2- and 3-digit numbers 

1189 Add and subtract 3- and 4-digit numbers 

1190 Multiply 1-digit with 2- or 3-digit multiples of 10 

1191 Multiply 1-digit with 2- or 3-digit multiples of 10 or 100 

1192 Divide 3-digit multiples of 10 by 1-digit numbers 

1193 Add and subtract 4- and 5-digit numbers 

1194 Multiply and divide multiples of 10 

1195 Add and subtract fractions with like denominators 

1196 Order of operations 

1197 Add and subtract fractions with unlike denominators 

1198 Multiply decimals (tenths) by whole numbers or 

1199 decimals (tenths) 

1200 Divide whole numbers by fractions 

1201 Solve for y in 1- or 2-step equations 

1202 Add and subtract negative numbers 

1203  

1204  

1205  

1206 2 

1207 36 

1208 6 

1209 -- 

1210 -- 

1211 -- 

1212 -- 

1213 -- 

1214 -- 

1215 -- 

1216 -- 

1217 -- 

1218 -- 

1219 -- 

1220 -- 

1221 -- 

1222 3 

1223 -- 

1224 -- 

1225 26 

1226 16 

1227 -- 

1228 -- 

1229 -- 

1230 -- 

1231 -- 

1232 -- 

1233 -- 

1234 -- 

1235 -- 

1236 -- 

1237 -- 

1238 Grade 

1239 4 

1240 -- 

1241 -- 

1242 -- 

1243 -- 

1244 9 

1245 6 

1246 5 

1247 -- 

1248 -- 

1249 -- 

1250 -- 

1251 -- 

1252 -- 

1253 27 

1254 16 

1255 -- 

1256 -- 

1257 -- 

1258 -- 

1259 -- 

1260 -- 

1261 -- 

1262 -- 

1263 8 

1264 6 

1265 6 

1266 6 

1267 -- 

1268 -- 

1269 -- 

1270 -- 

1271 6 

1272 -- 

1273 -- 

1274 -- 

1275 -- 

1276 -- 

1277 -- 

1278 -- 

1279 9 

1280 3 

1281 12 

1282 10 

1283 8 

1284 -- 

1285 -- 

1286 -- 

1287 7 

1288 -- 

1289 -- 

1290 -- 

1291 -- 

1292 -- 

1293 -- 

1294 -- 

1295 -- 

1296 -- 

1297 12 

1298 10 

1299 5 

1300 4 

1301 11 

1302 -- 

1303 42 

1304 8 

1305 -- 

1306 -- 

1307 -- 

1308 -- 

1309 -- 

1310 -- 

1311 -- 

1312 -- 

1313 -- 

1314 -- 

1315 10 

1316 4 

1317 6 

1318 16 

1319 6 

1320 42 

1321 Item total 

1322 42 

1323 42 

1324 42 

1325 42 

1326 42 

1327 aimswebPlus 

1328 For more information visit aimswebPlus.com 

1329 |   21   | 

1330  

1331 Progress Monitoring Guide 

1332 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1333 References 

1334  

1335 Black, P., & Wiliam, D. (1998). Assessment and classroom learning. Assessment in Education, 5(1), 7–74. 

1336  

1337 Christ, T. J., Zopluoglu, C., Long, J. D., & Monaghen, B. D. (2012). Curriculum-based measurement of oral 

1338 reading: Quality of progress monitoring outcomes. Exceptional Children, 78(3), 356–373. 

1339  

1340 Deno, S. L. (1985). Curriculum-based measurement: The emerging alternative. Exceptional Children, 52, 

1341 219–232. 

1342  

1343 Deno, S. L. (1986). Formative evaluation of individual student programs: A new role for school 

1344 psychologists. School Psychology Review, 15(3), 358–374. 

1345  

1346 Deno, S. L. (2003). Developments in curriculum-based measurement. The Journal of Special Education, 

1347 37(3), 184–192. 

1348  

1349 Every Student Succeeds Act of 2015, Pub. L. No. 114-95, 129 Stat. 1802 (2015). 

1350  

1351 Fuchs, L. S., & Fuchs, D. (1986). Effects of systematic formative evaluation: A meta-analysis. Exceptional 

1352 Children, 53(3), 199–208. 

1353  

1354 Individuals with Disabilities Education Improvement Act of 2004, Pub. L. No. 108-446, 118 Stat.  

1355 2647 (2004). 

1356  

1357 Pearson. (2012). aimsweb ROI Growth Norms Guide. Bloomington, MN: Author. 

1358  

1359 Shapiro, E. S. (2008). Best practices in setting progress monitoring goals for academic skill improvement.  

1360 In A. Thomas & J. Grimes (Eds.), Best practices in school psychology V (pp. 141–158). Bethesda, MD: 

1361 National Association of School Psychologists. 

1362  

1363 Silberglitt, B., & Hintze, J. M. (2007). How much growth can we expect? A conditional analysis of R–CBM 

1364 growth rates by level of performance. Exceptional Children, 74(1), 71–84. 

1365  

1366 Stecker, P. M., Fuchs, L. S., & Fuchs, D. (2005). Using curriculum-based measurement to improve student 

1367 achievement: Review of research. Psychology in the Schools 42(8), 795–819. 

1368  

1369  

1370  

1371 aimswebPlus 

1372 For more information visit aimswebPlus.com 

1373 |   22   | 

1374  

1375 Progress Monitoring Guide 

1376 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1377 Appendix A 

1378 Student Growth Percentiles 

1379 Performance-level percentiles and performance benchmarks are useful for defining meaningful 

1380 performance goals for individual students. However, before deciding on a goal for a given student, it is 

1381 desirable to evaluate the reasonableness of that goal. One way to determine this is to compare the rate of 

1382 improvement (ROI) needed to reach the goal with the ROIs of students who are starting out at about the 

1383 same level of performance. If the goal ROI for the student is high and rarely observed amongst his or her 

1384 peers, recognize that this goal will likely be very challenging. Similarly, a goal ROI that is commonly 

1385 achieved may be insufficient for a student receiving instructional intervention. 

1386  

1387 In aimswebPlus, a student’s ROI is the average increase in his or her raw score per week. The aimswebPlus 

1388 system calculates a student’s goal ROI (i.e., the rate of improvement needed to reach the goal) as follows: 

1389  

1390 𝑅𝑅𝑅𝑅𝑅𝑅=(𝐺𝐺𝐺𝐺𝐺𝐺𝐺𝐺 𝑆𝑆𝑆𝑆𝐺𝐺𝑆𝑆𝑆𝑆−𝑅𝑅𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐺𝐺𝐺𝐺 𝑆𝑆𝑆𝑆𝐺𝐺𝑆𝑆𝑆𝑆)

1391 𝑊𝑊𝑆𝑆𝑆𝑆𝑊𝑊𝑊𝑊 𝐸𝐸𝐺𝐺𝐺𝐺𝐸𝐸𝑊𝑊𝑆𝑆𝐸𝐸

1392  

1393  

1394 The initial score is the score at the start of the progress-monitoring period; often, this is the score from the 

1395 Fall or Winter benchmark testing. The goal score is the expected score at the end of the instructional 

1396 period, which is typically 1 academic year (or 36 weeks). The weeks elapsed is the number of weeks 

1397 between the dates of measurement. 

1398  

1399 To illustrate, take a Grade 4 student who obtained a Number Sense Fluency score of 10 during Fall 

1400 screening. The student’s teacher defined a Spring screening goal score of 28. Therefore, the goal ROI is: 

1401  

1402 𝑅𝑅𝑅𝑅𝑅𝑅=(28−10)

1403 36

1404 →1836→0.50 

1405  

1406 This student would need to gain 0.5 points per week on the NSF measure to reach the goal score of 28 by 

1407 Spring benchmark testing. 

1408  

1409 aimswebPlus’ student growth percentiles (SGPs) convey the reasonableness of ROIs by comparing them to 

1410 seasonal and annual growth rates of students with a similar initial score in the national sample. As such, 

1411 SGPs indicate whether growth is About Average, Above Average, or Below Average as compared to their 

1412 similarly performing peers. 

1413  

1414 In the above example, based on aimswebPlus national ROI growth norms, an ROI of 0.5 points per week 

1415 for a Grade 4 student is Above Average and corresponds to the 75th percentile when compared with 

1416 other students whose Fall screening scores were at a similar level. Growth rates in this range are labeled 

1417 Closes the Gap because they exceed the average growth rate and, if achieved, will move the student from 

1418 the 18th percentile in Fall screening to the 42nd percentile in Spring screening. 

1419  

1420 SGPs compare each student’s growth rate to students with a similar level of initial performance (e.g., Fall 

1421 screening score) because aimswebPlus researchers and others (Silberglitt & Hintze, 2007) have observed 

1422 that the average rate of growth is often related to the initial level of performance. In general, students with 

1423 very low initial performance levels tend to have lower ROIs, unless they are receiving supplemental 

1424 aimswebPlus 

1425 For more information visit aimswebPlus.com 

1426 |   23   | 

1427  

1428 Progress Monitoring Guide 

1429 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1430 instruction. Students with very high initial scores also tend to have relatively low ROIs, partly because of 

1431 regression effects and partly because their ability to demonstrate their skill may be limited by the score 

1432 range of the measure. By constructing separate SGPs for various levels of initial performance, the SGPs 

1433 adjust for any ROI differences caused by the initial performance level. (For details regarding ROI norms 

1434 development, see the aimsweb ROI Growth Norms Guide available at www.aimsweb.com.) 

1435  

1436 A key element to the success of a multi-tiered system of supports program is setting meaningful goals.  

1437 An important contribution of CBM progress monitoring is the articulation of performance goals that are 

1438 measurable, meaningful, and manageable. Within CBM progress monitoring, goals are measurable  

1439 because they are expressed in terms of the raw total score on each measure. The total score is directly 

1440 observable and easy to attain. For most aimswebPlus CBMs, the total score is equal to the number of 

1441 items correctly answered. 

1442  

1443 However, by themselves, raw scores have limited meaning. Without other criteria or lots of prior 

1444 experience, it is difficult to ascertain if a given raw score goal will be appropriately challenging for the 

1445 student and if it represents a level of achievement that closes the gap, putting the student back on track  

1446 for success. For these reasons, the aimswebPlus goal slider tool (see Figure 6) was designed to assist  

1447 the teacher in establishing meaningful goals. This goal slider provides three ways to interpret the 

1448 meaningfulness of the goal: 

1449 •  Raw total score (e.g., words read per minute for ORF) and corresponding national percentile for 

1450 the grade and season of the goal date 

1451 •  Color-coded horizontal scale using the corresponding national or local percentile 

1452 • 

1453 Feedback about the ambitiousness of goals, based on the national student growth percentiles (i.e., 

1454 Insufficient, Closes the Gap, Ambitious, Extremely Ambitious) 

1455 After a meaningful goal has been established, all of the student’s progress monitoring scores are 

1456 automatically loaded into his or her progress monitoring chart, making the charting process easy to 

1457 manage in the aimswebPlus system. 

1458  

1459  

1460  

1461 aimswebPlus 

1462 For more information visit aimswebPlus.com 

1463 |   24   | 

1464  

1465 Progress Monitoring Guide 

1466 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1467 Appendix B 

1468 Visual Analysis 

1469 Visual analysis should be a part of every progress monitoring data review. Although subjective, a visual 

1470 analysis is relatively easy to conduct and enables the user to account for unusual characteristics in the 

1471 pattern of scores that may interfere with interpreting the projection line. However, it is important to  

1472 note that visual analysis does not give a numerical answer to the question of whether a student is likely to 

1473 reach his or her goal. 

1474  

1475 An outlier—an individual score that is highly discrepant from the general trend—can be easily identified on 

1476 visual inspection. Outliers can significantly affect the projection line, especially when they occur near the 

1477 beginning or the end of the progress monitoring period. If the user determines that an outlying score likely 

1478 is not a valid indicator of the student’s true ability at that point in time (e.g., the student was ill or 

1479 distracted, a procedural flaw in administration or scoring occurred), then making a mental adjustment of 

1480 the projection line is appropriate. 

1481  

1482 Figures B1 and B2 illustrate the effect of a low-scoring outlier on the projection line. In Figure B1, the 

1483 Week 6 outlier lowers the projection line, suggesting that the student will not meet his or her goal at the 

1484 present rate of growth. When the outlier is removed, the projection line steepens, indicating that the 

1485 student is likely to meet the goal at the present growth rate (Figure B2). 

1486  

1487 Figure B1  Projection Line With Outlier 

1488  

1489 Figure B2  Projection Line With Outlier Removed 

1490  

1491  

1492  

1493 aimswebPlus 

1494 For more information visit aimswebPlus.com 

1495 |   25   | 

1496  

1497 Progress Monitoring Guide 

1498 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1499  

1500  

1501 Similarly, a nonlinear pattern of score growth can make the projection line a less accurate description of  

1502 the student’s progress and a predictor of future performance. Figure B3 illustrates a curved pattern of 

1503 score growth, with the linear projection line superimposed. Although the curved shape of the growth 

1504 trend often is not as clear cut as illustrated here, the user should take apparent nonlinearity into account  

1505 when interpreting the results of other data analysis methods, including rules of thumb, projection lines, and 

1506 confidence intervals. 

1507  

1508 Figure B3  Projection Line With Nonlinear Pattern of Score Growth 

1509  

1510 Visual analysis is also useful for identifying the amount of random variability of scores around the trend. Be 

1511 aware that some students show a more consistent set of scores than others do, meaning their scores stay 

1512 closer to the growth trend. Conversely, other students perform erratically, which could be the result of 

1513 gaps in skill development, variable levels of motivation, and/or inconsistent measurement conditions. 

1514  

1515 When a large amount of “scatter” is apparent in the scores, discerning the trend can be difficult. The more 

1516 variable the scores are, the less accurate the prediction of future performance can be. Figures B4 and B5 

1517 illustrate two sets of scores, one with little scatter (Figure B4) and the other with a large amount of scatter 

1518 (Figure B5). Both show a linear growth pattern; however, it is more difficult to discern the pattern when 

1519 more scatter is apparent in the scores, as shown in Figure B5. 

1520  

1521 Figure B4  Score Pattern With Low Variability 

1522  

1523  

1524  

1525 aimswebPlus 

1526 For more information visit aimswebPlus.com 

1527 |   26   | 

1528  

1529 Progress Monitoring Guide 

1530 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1531  

1532  

1533 Figure B5  Score Pattern With High Variability 

1534  

1535  

1536 aimswebPlus 

1537 For more information visit aimswebPlus.com 

1538 |   27   | 

1539  

1540 Progress Monitoring Guide 

1541 Copyright © 2017 NCS Pearson, Inc. All rights reserved. 

1542 Pearson Executive Office 5601 Green Valley Drive Bloomington, MN 55437 

1543 800.627.7271  www.aimsweb.com 

1544 Copyright ©2017 NCS Pearson, Inc. All rights reserved. 

1545 Warning: No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopy,  

1546 recording, or any information storage and retrieval system, without permission in writing from the copyright owner.

1547 Pearson, aimswebPlus, and the aimswebPlus logo are trademarks in the U.S. and/or other countries, of Pearson Education, Inc., or its affiliate(s). 

1548 Produced in the United States of America.

1549 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

